# Rodrigo Martins Ceia n¬∫2023222356
# Gabriel Alexandre Sabino Costeira n¬∫ 2023222421
import os
import csv
import warnings
import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import NearestNeighbors
import random
from scipy.stats import ttest_rel

# --- Novos Imports para Ex 3 ---
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, classification_report

# --- Imports para Embeddings (Exerc√≠cio 2) ---
try:
    import torch
    # Importar fun√ß√µes do ficheiro auxiliar
    from embeddings_extractor import load_model, resample_to_30hz_5s, acc_segmentation
    HAS_TORCH = True
except ImportError as e:
    HAS_TORCH = False
    print(f"AVISO: Falha ao importar torch ou embeddings_extractor: {e}")
    print("O Exerc√≠cio 2 ser√° ignorado.")

warnings.filterwarnings("ignore", category=RuntimeWarning)

# ------------------------------
# Constantes
# ------------------------------
IDX_PART = 0
IDX_LABEL = 1
IDX_FEATS = 2

# ------------------------------
# Utilit√°rios de sa√≠da
# ------------------------------

def ensure_outputs_dir(dirname="outputs"):
    os.makedirs(dirname, exist_ok=True)
    return dirname

def savefig(path):
    plt.tight_layout()
    plt.savefig(path, dpi=150)
    plt.close()

def save_csv(path, header, rows):
    ensure_outputs_dir(os.path.dirname(path))
    with open(path, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        if header: writer.writerow(header)
        writer.writerows(rows)

outdir = ensure_outputs_dir("outputsB")

# ------------------------------
# 0. Fun√ß√µes de Carregamento
# ------------------------------

def carregar_dados_brutos(part_id, pasta_base="FORTH_TRACE_DATASET", devices=(1,2,3,4,5)):
    rows = []
    for dev in devices:
        path = os.path.join(pasta_base, f"part{part_id}", f"part{part_id}dev{dev}.csv")
        if not os.path.isfile(path): continue
        try:
            with open(path, "r") as f:
                reader = csv.reader(f)
                for line in reader:
                    if not line: continue
                    try:
                        vals = [float(x) for x in line]
                        if vals[-1] <= 7: rows.append(vals)
                    except ValueError: continue 
        except Exception as e:
            print(f"Erro ao ler {path}: {e}")
    return np.array(rows) if rows else None

def load_features_dataset(file_path):
    """
    Vers√£o Robusta: Limpa caracteres '[' e ']' que possam existir no CSV.
    """
    if not os.path.exists(file_path):
        print(f"ERRO: '{file_path}' n√£o existe.")
        return None, None
    try:
        with open(file_path, 'r') as f:
            lines = f.readlines()
            
        if not lines: return None, None
        
        # 1. Cabe√ßalho
        header = lines[0].strip().split(',')
        
        # 2. Dados (Parsing manual para remover par√™ntesis)
        data = []
        for line in lines[1:]:
            line = line.strip()
            if not line: continue
            
            # Remove caracteres problem√°ticos
            clean_line = line.replace('[', '').replace(']', '').replace('"', '')
            try:
                vals = [float(x) for x in clean_line.split(',')]
                data.append(vals)
            except ValueError:
                continue
                
        return header, np.array(data)

    except Exception as e:
        print(f"Erro ao carregar dataset: {e}")
        return None, None

# ------------------------------
# 1.1 Plots
# ------------------------------
def plot_class_balance(labels_all, output_path, title="Distribui√ß√£o", ylabel="Contagem"):
    classes, counts = np.unique(labels_all, return_counts=True)
    print(f"\nContagem ({ylabel}):")
    for c, n in zip(classes, counts): print(f"Ativ {int(c)}: {n}")
    
    if len(counts) > 0:
        r = np.max(counts) / np.min(counts)
        print(f"Ratio Max/Min: {r:.2f}")

    plt.figure(figsize=(10, 6))
    plt.bar(classes, counts, color='skyblue', edgecolor='black')
    plt.title(title); plt.xlabel("Atividade"); plt.ylabel(ylabel)
    plt.xticks(classes); plt.grid(axis='y', linestyle='--', alpha=0.7)
    savefig(output_path)

# ------------------------------
# 1.2 e 1.3 SMOTE Logic
# ------------------------------
def generate_smote_samples(features_data, k_samples, k_neighbors=5):
    if len(features_data) < 2: return None
    k_neighbors = min(len(features_data) - 1, k_neighbors)
    nbrs = NearestNeighbors(n_neighbors=k_neighbors + 1).fit(features_data)
    _, indices = nbrs.kneighbors(features_data)
    
    new_samples = []
    for _ in range(k_samples):
        base = features_data[random.randint(0, len(features_data) - 1)]
        neighbor = features_data[indices[random.randint(0, len(features_data) - 1)][random.randint(1, k_neighbors)]]
        new_samples.append(base + random.random() * (neighbor - base))
    return np.array(new_samples)

def plot_smote_visualization(data_participant, synthetic_features, activity_target, feat_names, output_path):
    f1, f2 = feat_names
    plt.figure(figsize=(10, 7))
    for lbl in np.unique(data_participant[:, IDX_LABEL]):
        subset = data_participant[data_participant[:, IDX_LABEL] == lbl][:, IDX_FEATS:]
        alpha = 0.3 if lbl != activity_target else 0.6
        plt.scatter(subset[:, 0], subset[:, 1], label=f"Ativ {int(lbl)}", alpha=alpha, s=40)
    
    plt.scatter(synthetic_features[:, 0], synthetic_features[:, 1], c='black', marker='X', s=150, label='SMOTE', edgecolors='white')
    plt.title(f"SMOTE (Ativ {activity_target}) - {f1} vs {f2}")
    plt.xlabel(f1); plt.ylabel(f2); plt.legend(); plt.grid(True, alpha=0.5)
    savefig(output_path)

# =============================================================================
# EXERC√çCIO 2.1: EMBEDDINGS EXTRACTOR (Op√ß√£o C - Stacking)
# =============================================================================

def gerar_embeddings_dataset(participantes, pasta_base, output_csv):
    """
    Extrai embeddings usando 'embeddings_extractor.py' (harnet5).
    Usa Op√ß√£o C: Stacking (cada device √© tratado como uma sample independente).
    """
    if not HAS_TORCH: return

    print("A carregar modelo de embeddings (harnet5)...")
    try:
        encoder = load_model() # Importado
    except Exception as e:
        print(f"Erro ao carregar modelo: {e}")
        return

    device = "cuda" if torch.cuda.is_available() else "cpu"
    encoder.to(device)
    
    print(f"A extrair embeddings (Stacking) em {device}...")
    fs_original = 50.0
    all_rows = []

    # Loop: Participantes -> Devices
    for p_id in participantes:
        for dev_id in [1, 2, 3, 4, 5]:
            path_csv = os.path.join(pasta_base, f"part{p_id}", f"part{p_id}dev{dev_id}.csv")
            if not os.path.isfile(path_csv): continue

            try:
                csv_data = np.loadtxt(path_csv, delimiter=',')
            except Exception: continue

            try:
                segments, activities = acc_segmentation(csv_data)
            except Exception as e:
                print(f"Erro na segmenta√ß√£o P{p_id}D{dev_id}: {e}")
                continue

            if not segments: continue

            batch_tensors = []
            batch_meta = [] 

            for seg, act in zip(segments, activities):
                if act > 7: continue 

                seg_30hz, _ = resample_to_30hz_5s(seg, fs_original)
                batch_tensors.append(seg_30hz)
                batch_meta.append([p_id, act])
            
            if not batch_tensors: continue

            x_np = np.array(batch_tensors) # (B, 150, 3)
            x_np = np.transpose(x_np, (0, 2, 1)) # (B, 3, 150)
            
            MINI_BATCH = 32
            n_total = len(x_np)
            
            with torch.no_grad():
                for i in range(0, n_total, MINI_BATCH):
                    x_batch = x_np[i : i + MINI_BATCH]
                    x_tensor = torch.from_numpy(x_batch).float().to(device)
                    emb_batch = encoder(x_tensor).cpu().numpy() 

                    for j, emb_vec in enumerate(emb_batch):
                        meta = batch_meta[i + j]
                        all_rows.append( meta + emb_vec.tolist() )
            
            print(f" -> P{p_id} Dev{dev_id}: {len(batch_tensors)} segmentos processados.")

    if all_rows:
        n_dim = len(all_rows[0]) - 2
        header = ["participante", "label"] + [f"emb_{k}" for k in range(n_dim)]
        save_csv(output_csv, header, all_rows)
        print(f"\nSUCESSO: {len(all_rows)} embeddings guardados em '{output_csv}'.")
    else:
        print("Aviso: Nenhum embedding gerado.")

# =============================================================================
# EXERC√çCIO 3: SPLITTING E PIPELINE
# =============================================================================

# --- Fun√ß√£o ReliefF (necess√°ria para o Cen√°rio C) ---
def reliefF(X, y, n_neighbors=10, n_samples=200):
    """
    Implementa√ß√£o simplificada do ReliefF para sele√ß√£o de features.
    """
    rng = np.random.RandomState(0)
    X = np.asarray(X, dtype=float)
    y = np.asarray(y)
    n, d = X.shape

    # Amostrar um subconjunto para efici√™ncia
    m = min(n_samples, n)
    idx_s = rng.choice(n, m, replace=False)

    scores = np.zeros(d)
    
    # Nearest Neighbors global para encontrar hits/misses
    nbrs = NearestNeighbors(n_neighbors=n_neighbors+1).fit(X)
    
    for i in idx_s:
        xi, yi = X[i], y[i]
        
        # Encontrar vizinhos no dataset todo
        dists, inds = nbrs.kneighbors([xi])
        # inds[0][0] √© o pr√≥prio ponto, ent√£o pegamos do 1 em diante
        neighbors_idx = inds[0][1:]
        
        # Calcular update de scores
        # (Simplifica√ß√£o: ReliefF cl√°ssico busca k hits e k misses separadamente.
        #  Aqui usamos vizinhan√ßa geral e penalizamos/premiamos conforme a classe)
        
        for n_idx in neighbors_idx:
            xn = X[n_idx]
            yn = y[n_idx]
            
            dist_feat = np.abs(xi - xn)
            
            # Normalizar diff se poss√≠vel (assumindo dados normalizados)
            
            if yi == yn: # Hit (mesma classe) -> penaliza features que s√£o diferentes
                scores -= dist_feat
            else: # Miss (classe diferente) -> premia features que s√£o diferentes
                scores += dist_feat

    return scores


# --- 3.1 Within-Subject Split ---
def split_within_subjects(X, y, participants, seed=42):
    """
    Divide 60% Treino, 20% Val, 20% Teste, misturando dados de todos os participantes.
    """
    # 1. Separa Treino (60%) do Resto (40%)
    X_train, X_temp, y_train, y_temp, p_train, p_temp = train_test_split(
        X, y, participants, test_size=0.4, random_state=seed, stratify=y
    )
    # 2. Separa Resto em Valida√ß√£o (metade de 40% -> 20%) e Teste (20%)
    X_val, X_test, y_val, y_test, p_val, p_test = train_test_split(
        X_temp, y_temp, p_temp, test_size=0.5, random_state=seed, stratify=y_temp
    )
    
    print(f"   [Split 3.1 Within] Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)}")
    return X_train, y_train, X_val, y_val, X_test, y_test

# --- 3.2 Between-Subject Split ---
def split_between_subjects(X, y, participants, seed=42):
    """
    Divide por participante: 9 Treino, 3 Valida√ß√£o, 3 Teste.
    """
    unique_parts = np.unique(participants)
    np.random.seed(seed)
    np.random.shuffle(unique_parts)
    
    n_total = len(unique_parts)
    if n_total < 3: return None
        
    n_train = 9 if n_total >= 15 else int(n_total * 0.6)
    n_val = 3 if n_total >= 15 else int(n_total * 0.2)
    
    train_ids = unique_parts[:n_train]
    val_ids = unique_parts[n_train : n_train + n_val]
    test_ids = unique_parts[n_train + n_val :]
    
    mask_train = np.isin(participants, train_ids)
    mask_val = np.isin(participants, val_ids)
    mask_test = np.isin(participants, test_ids)
    
    print(f"   [Split 3.2 Between] Train IDs: {train_ids} | Val: {val_ids} | Test: {test_ids}")
    return X[mask_train], y[mask_train], X[mask_val], y[mask_val], X[mask_test], y[mask_test]


# --- 3.4 Pipeline ---
def process_pipeline_scenarios(X_train, X_val, X_test, y_train):
    """
    Gera os cen√°rios A (Normal), B (PCA 90%) e C (ReliefF Top 15).
    """
    results = {}
    
    # 0. Normaliza√ß√£o (Fit apenas no Treino)
    scaler = StandardScaler()
    X_train_norm = scaler.fit_transform(X_train)
    X_val_norm = scaler.transform(X_val)
    X_test_norm = scaler.transform(X_test)
    
    # Cen√°rio A: All Features (Normalizado)
    results['A'] = (X_train_norm, X_val_norm, X_test_norm)
    
    # Cen√°rio B: PCA (90% Vari√¢ncia)
    # Nota: fit apenas no treino normalizado
    pca = PCA(n_components=0.90, random_state=42)
    X_train_pca = pca.fit_transform(X_train_norm)
    X_val_pca = pca.transform(X_val_norm)
    X_test_pca = pca.transform(X_test_norm)
    
    results['B'] = (X_train_pca, X_val_pca, X_test_pca)
    print(f"      -> Cen√°rio B (PCA): {X_train.shape[1]} dims -> {pca.n_components_} componentes.")
    
    # Cen√°rio C: ReliefF (Top 15 Features)
    # Usar apenas o treino para calcular scores
    print("      -> Cen√°rio C (ReliefF): A calcular scores (pode demorar)...")
    scores = reliefF(X_train_norm, y_train, n_neighbors=10, n_samples=300)
    
    # Selecionar Top 15 √≠ndices
    top_15_idx = np.argsort(scores)[::-1][:15]
    
    X_train_relief = X_train_norm[:, top_15_idx]
    X_val_relief = X_val_norm[:, top_15_idx]
    X_test_relief = X_test_norm[:, top_15_idx]
    
    results['C'] = (X_train_relief, X_val_relief, X_test_relief)
    print(f"      -> Cen√°rio C (ReliefF): Selecionadas as 15 melhores features.")
    
    return results

# =============================================================================
# EXERC√çCIO 4: MODEL LEARNING (k-NN)
# =============================================================================

# --- 4.1 Implementa√ß√£o Customizada do k-NN ---
class MyKNN:
    """
    Implementa√ß√£o manual de k-NN para satisfazer o requisito 4.1.
    Usa dist√¢ncia euclidiana e vota√ß√£o por maioria.
    """
    def __init__(self, k=3):
        self.k = k
        self.X_train = None
        self.y_train = None

    def fit(self, X, y):
        """Armazena os dados de treino."""
        self.X_train = np.array(X)
        self.y_train = np.array(y)

    def predict(self, X_test):
        """Calcula dist√¢ncias e devolve predi√ß√µes."""
        predictions = []
        X_test = np.array(X_test)
        
        for row_test in X_test:
            # 1. Dist√¢ncia Euclidiana (Broadcasting)
            # sqrt(sum((x_train - x_test)^2))
            dists = np.linalg.norm(self.X_train - row_test, axis=1)
            
            # 2. Encontrar os √≠ndices dos k vizinhos mais pr√≥ximos (menores dist√¢ncias)
            k_indices = np.argsort(dists)[:self.k]
            
            # 3. Obter as labels desses vizinhos
            k_nearest_labels = self.y_train[k_indices]
            
            # 4. Vota√ß√£o por maioria
            unique, counts = np.unique(k_nearest_labels, return_counts=True)
            most_common = unique[np.argmax(counts)]
            predictions.append(most_common)
            
        return np.array(predictions)

# --- 4.2 M√©tricas de Avalia√ß√£o ---
def calculate_metrics(y_true, y_pred, set_name="Validation"):
    """
    Calcula e imprime m√©tricas de classifica√ß√£o: Accuracy, F1, Precision, Recall e Matriz Confus√£o.
    """
    acc = accuracy_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)
    prec = precision_score(y_true, y_pred, average='weighted', zero_division=0)
    rec = recall_score(y_true, y_pred, average='weighted', zero_division=0)
    cm = confusion_matrix(y_true, y_pred)
    
    print(f"\n   --- M√©tricas [{set_name}] ---")
    print(f"   Accuracy:  {acc:.4f}")
    print(f"   F1-Score:  {f1:.4f} (Weighted)")
    print(f"   Precision: {prec:.4f}")
    print(f"   Recall:    {rec:.4f}")
    print(f"   Matriz de Confus√£o:\n{cm}")
    
    return acc, f1

# =============================================================================
# EXERC√çCIO 5.1: MODEL LEARNING (k-NN)
# =============================================================================

def tune_and_retrain(X_train, y_train, X_val, y_val, X_test, y_test, k_values=[1,3,5,7,9,11,13]):
    """
    Exerc√≠cio 5.1:
    1. Tuning: Encontrar melhor k usando Valida√ß√£o.
    2. Retrain: Treinar com (Treino + Valida√ß√£o) usando melhor k.
    3. Avalia√ß√£o: Testar no conjunto de Teste.
    """
    # A. Tuning
    best_acc = -1
    best_k = k_values[0]
    
    for k in k_values:
        clf = KNeighborsClassifier(n_neighbors=k)
        clf.fit(X_train, y_train)
        # Score devolve a accuracy diretamente
        acc = clf.score(X_val, y_val)
        
        if acc > best_acc:
            best_acc = acc
            best_k = k
            
    # B. Retrain (Juntar dados)
    X_full_train = np.vstack((X_train, X_val))
    y_full_train = np.concatenate((y_train, y_val))
    
    # C. Modelo Final
    final_clf = KNeighborsClassifier(n_neighbors=best_k)
    final_clf.fit(X_full_train, y_full_train)
    
    # D. Avalia√ß√£o Final
    y_pred_test = final_clf.predict(X_test)
    test_acc = accuracy_score(y_test, y_pred_test)
    
    # DEVOLVE 4 VALORES: Accuracy, K, True Labels, Predicted Labels
    return test_acc, best_k, y_test, y_pred_test


def perform_hypothesis_testing(results_dict):
    """
    5.3: Compara√ß√£o Estat√≠stica usando Paired T-Test.
    Recebe: {'ModelName': [acc_iter1, acc_iter2, ...], ...}
    """
    print("\n" + "="*60)
    print("=== 5.3 TESTES DE HIP√ìTESE (Statistical Significance) ===")
    print("="*60)
    
    if len(results_dict) < 2:
        print("Menos de 2 modelos para comparar. Ignorando.")
        return

    # 1. Determinar o Melhor Modelo (Baseline)
    model_means = {m: np.mean(s) for m, s in results_dict.items()}
    best_model_name = max(model_means, key=model_means.get)
    best_scores = results_dict[best_model_name]
    
    print(f"üèÜ MELHOR MODELO (Baseline): {best_model_name}")
    print(f"   M√©dia Accuracy: {model_means[best_model_name]:.4f}")
    print(f"   Desvio Padr√£o:  {np.std(best_scores):.4f}")
    print("-" * 60)
    print(f"{'Modelo Comparado':<30} | {'M√©dia':<8} | {'p-value':<10} | {'Significativo?'}")
    print("-" * 60)

    # 2. Comparar o Melhor contra os Restantes
    alpha = 0.05 # N√≠vel de signific√¢ncia (95% confian√ßa)
    
    for model_name, scores in results_dict.items():
        if model_name == best_model_name:
            continue
            
        # Teste T Emparelhado (Paired T-Test)
        # Usamos emparelhado porque ambos os modelos foram testados 
        # EXATAMENTE nas mesmas divis√µes (seeds) de dados.
        t_stat, p_val = ttest_rel(best_scores, scores)
        
        is_significant = "SIM ‚úÖ" if p_val < alpha else "N√ÉO ‚ùå"
        
        print(f"{model_name:<30} | {np.mean(scores):.4f}   | {p_val:.2e}   | {is_significant}")

    print("-" * 60)
    print("Justifica√ß√£o Estat√≠stica:")
    print("Utilizou-se o 'Paired Samples t-test' (Teste T Emparelhado).")
    print("Justifica√ß√£o: As amostras de performance n√£o s√£o independentes, pois todos os")
    print("modelos foram avaliados sobre as mesmas repeti√ß√µes de splits (mesmas seeds).")
    print("Isto reduz a vari√¢ncia explicada pelo split, focando na diferen√ßa real dos modelos.")
    print("="*60 + "\n")



# ------------------------------
# Main
# ------------------------------

def main():
    # --- EXERC√çCIO 1.1 ---
    print("\n=== 1.1 Balan√ßo de Classes (Dados Brutos) ===")
    PASTA_RAW = "FORTH_TRACE_DATASET"
    PARTICIPANTES = list(range(0, 15)) # 0 a 14
    
    raw_labels = []
    for p in PARTICIPANTES:
        d = carregar_dados_brutos(p, PASTA_RAW)
        if d is not None: raw_labels.append(d[:, -1])
    
    if raw_labels:
        all_labels = np.concatenate(raw_labels)
        plot_class_balance(all_labels, os.path.join(outdir, "1_1_balance_raw.png"), "Distribui√ß√£o Raw")
    
    # --- EXERC√çCIO 1.3 (SMOTE) ---
    print("\n=== 1.3 SMOTE (Features) ===")
    path_feats = os.path.join("outputs", "4_2_features_windows.csv")
    h, data = load_features_dataset(path_feats)
    
    if data is not None:
        data = data[data[:, IDX_LABEL] <= 7] # Filtro
        
        # P3, Ativ 4
        p3_data = data[data[:, IDX_PART] == 3]
        p3_act4 = p3_data[p3_data[:, IDX_LABEL] == 4]
        
        if len(p3_act4) > 1:
            feats = p3_act4[:, IDX_FEATS:]
            synth = generate_smote_samples(feats, k_samples=3)
            if synth is not None:
                plot_smote_visualization(p3_data, synth, 4, (h[IDX_FEATS], h[IDX_FEATS+1]), os.path.join(outdir, "1_3_smote.png"))
        else:
            print("Dados insuficientes para SMOTE (P3, Ativ4).")

    # --- EXERC√çCIO 2.1 (Embeddings) ---
    print("\n=== 2.1 Embeddings Dataset (Stacking) ===")
    path_embed = os.path.join(outdir, "EMBEDDINGS_DATASET.csv")
    
    if not os.path.exists(path_embed):
        gerar_embeddings_dataset(PARTICIPANTES, PASTA_RAW, path_embed)
    else:
        print(f"Ficheiro '{path_embed}' j√° existe. A saltar extra√ß√£o.")

    # =================================================================
    # EXERC√çCIO 3: PIPELINE (FEATURES + EMBEDDINGS)
    # =================================================================
    print("\n=== 3. Pipeline (Splitting & PCA) ===")
    

    # Preparar Datasets
    # data (Features) j√° est√° carregado. Carregamos Embeddings.
    data_features = data 
    _, data_embeds = load_features_dataset(path_embed)
    
    if data_embeds is not None:
        data_embeds = data_embeds[data_embeds[:, IDX_LABEL] <= 7]

    datasets_to_process = []
    if data_features is not None: datasets_to_process.append(("FEATURES", data_features))
    if data_embeds is not None: datasets_to_process.append(("EMBEDDINGS", data_embeds))

    ready_data = {} # Guarda dados prontos para treino

    for name, ds in datasets_to_process:
        print(f"\n>>> Processando {name} ({len(ds)} amostras)...")
        
        X = ds[:, IDX_FEATS:]
        y = ds[:, IDX_LABEL]
        p = ds[:, IDX_PART]
        
        # 3.1 Split Within-Subjects
        print(" -> Estrat√©gia 3.1 (Within):")
        split_within_subjects(X, y, p)
        
        # 3.2 Split Between-Subjects (Preferida)
        print(" -> Estrat√©gia 3.2 (Between):")
        split_res = split_between_subjects(X, y, p)
        
        if split_res is not None:
            X_tr, y_tr, X_val, y_val, X_te, y_te = split_res
            
            # 3.4 Pipeline (Aplicado apenas √† estrat√©gia 3.2)
            print(" -> Aplicando Pipeline √† Estrat√©gia 3.2:")
            scenarios = process_pipeline_scenarios(X_tr, X_val, X_te, y_tr)
            
            ready_data[name] = {
                'y': (y_tr, y_val, y_te),
                'scenarios': scenarios
            }
        
    print("\nPipeline conclu√≠da! Dados prontos em 'ready_data'.")

    # =================================================================
    # EXERC√çCIO 4 & 5.1: MODEL LEARNING & EVALUATION
    # =================================================================
    print("\n=== 5. Evaluation Loop (Tuning, Retrain & Stats) ===")
    
    # Aumentar N para ter validade estat√≠stica (m√≠nimo 5, ideal 10-30)
    N_REPEATS = 5 
    K_VALUES_LIST = [1, 3, 5, 7, 9, 11, 13, 15]
    
    # Dicion√°rio para acumular resultados: {'FEATURES-A': [0.62, 0.61...], ...}
    results_history = {}

    for i in range(N_REPEATS):
        current_seed = 42 + i
        print(f"\n>> Itera√ß√£o {i+1}/{N_REPEATS} (Seed {current_seed})...")
        
        for ds_name in ready_data:
            # Recuperar dados brutos para fazer novo split
            # Nota: ready_data guardava o split fixo da seed 42. 
            # Para o Ex 5, precisamos de refazer o split com 'current_seed'.
            
            # Temos de ir buscar aos dados originais carregados no in√≠cio da main
            if ds_name == "FEATURES": original_data = data_features
            else: original_data = data_embeds
            
            X = original_data[:, IDX_FEATS:]
            y = original_data[:, IDX_LABEL]
            p = original_data[:, IDX_PART]
            
            # 1. Split Vari√°vel (Seed muda a cada itera√ß√£o)
            split_res = split_between_subjects(X, y, p, seed=current_seed)
            if split_res is None: continue
            X_tr, y_tr, X_val, y_val, X_te, y_te = split_res
            
            # 2. Pipeline
            scenarios = process_pipeline_scenarios(X_tr, X_val, X_te, y_tr)
            
            for sc_name, sc_data in scenarios.items():
                if sc_data is None: continue
                
                X_train_s, X_val_s, X_test_s = sc_data
                
                # 5.1 Tuning & Retrain
                acc_test, best_k, _, _ = tune_and_retrain(
                    X_train_s, y_tr, X_val_s, y_val, X_test_s, y_te, 
                    k_values=K_VALUES_LIST
                )
                
                # Guardar hist√≥rico
                key = f"{ds_name} - {sc_name}"
                if key not in results_history: results_history[key] = []
                results_history[key].append(acc_test)
                
                print(f"   [{key}] k={best_k}, Acc={acc_test:.4f}")

    # 5.3 Testes de Hip√≥tese
    perform_hypothesis_testing(results_history)

    print("\nPipeline Final Conclu√≠da!")


if __name__ == "__main__":
    main()